{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Clustering\n","Here we will clean the dataset of the News Channels that are not in english and generally not in the US\n","\n","Then we try to find communities within our filtered dataset.\n","\n","To detect the communites we use the Louvain Algorithm"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import networkx as nx\n","import networkx.algorithms.community as nx_comm\n","import pandas as pd\n"]},{"cell_type":"markdown","metadata":{},"source":["## Load the graph generated previously"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>source</th>\n","      <th>target</th>\n","      <th>weight</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>12</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   source  target  weight\n","0       0       1       2\n","1       0       2       1\n","2       0       3       5\n","3       0       4       3\n","4       0      12       1"]},"metadata":{},"output_type":"display_data"}],"source":["df_edges = pd.read_csv(\"data/graph.csv\", sep=';')\n","display(df_edges.head())\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# convert to networkx graph\n","G = nx.from_pandas_edgelist(df_edges, edge_attr=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Louvain algorithm on the unfiltered data\n","\n","We chose the parameters for the algorithm so that it favors larger communities"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["louvain_partitions = nx_comm.louvain_communities(G, resolution=1e-1, threshold=1e-100,seed=1)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Community number 0 has 83 members\n","Community number 1 has 3 members\n","Community number 2 has 243 members\n","Community number 3 has 2 members\n","Community number 4 has 15 members\n","Community number 5 has 6 members\n","Community number 6 has 2 members\n","Community number 7 has 2 members\n","Community number 8 has 2 members\n","Community number 9 has 3 members\n","Community number 10 has 5 members\n","Community number 11 has 2 members\n"]}],"source":["for idx in range(len(louvain_partitions)):\n","    print(\"Community number {i} has {number} members\".format(i=idx,number=len(louvain_partitions[idx])))"]},{"cell_type":"markdown","metadata":{},"source":["Now we print the contents of the 2 largest community numbers. We have not printed all of them here for brevity. Since we swapped the channel id strings for our channel id numbers, to find a channel id you need to : take the channel number go to the channels.csv file and find the corresponding channel id. Then type https://www.youtube.com/channel/\"channel_id\" to find the channel"]},{"cell_type":"markdown","metadata":{},"source":["The first community contains channel like IndiaTV or ABP News which are not in the scope of what we want to do."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{0, 256, 1, 3, 4, 2, 776, 12, 13, 272, 1040, 18, 19, 535, 25, 28, 30, 34, 36, 292, 39, 43, 44, 45, 47, 303, 51, 54, 316, 60, 317, 63, 65, 70, 327, 329, 79, 595, 84, 86, 89, 94, 98, 99, 100, 357, 359, 104, 105, 112, 113, 133, 134, 395, 654, 146, 149, 150, 156, 159, 418, 420, 170, 172, 686, 174, 178, 440, 702, 447, 451, 454, 456, 461, 206, 464, 470, 216, 220, 991, 236, 237, 254}\n"]}],"source":["print(louvain_partitions[0])"]},{"cell_type":"markdown","metadata":{},"source":["In this community we have what we are interested in. Channel number 6 is CNN and all the channels that we have checked ~60 out of 243 are all in english.\n","\n","For the next part we will use these channels for our graph."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{2049, 1027, 517, 6, 7, 8, 9, 10, 519, 11, 2061, 1039, 16, 15, 530, 529, 21, 534, 23, 539, 29, 31, 32, 546, 37, 2086, 554, 53, 1077, 56, 1084, 61, 62, 64, 576, 1602, 67, 582, 583, 1611, 76, 2124, 78, 1107, 1110, 87, 88, 90, 92, 93, 96, 97, 611, 612, 617, 1133, 109, 1134, 622, 626, 627, 1141, 629, 120, 633, 1146, 122, 121, 125, 634, 639, 645, 137, 651, 1163, 141, 142, 140, 1680, 145, 144, 147, 148, 661, 1165, 152, 1177, 153, 155, 1180, 672, 1185, 165, 1190, 169, 2218, 173, 176, 691, 692, 179, 1203, 1205, 187, 701, 191, 192, 195, 1220, 709, 199, 200, 201, 202, 203, 714, 717, 2255, 207, 208, 209, 723, 211, 722, 212, 215, 725, 221, 1249, 741, 229, 744, 238, 240, 754, 1778, 257, 770, 259, 264, 265, 266, 267, 1800, 1806, 273, 276, 1813, 278, 280, 794, 803, 293, 1833, 813, 307, 308, 309, 310, 824, 313, 1337, 315, 314, 318, 1349, 328, 331, 333, 1360, 339, 852, 341, 342, 855, 340, 859, 1377, 1379, 867, 360, 365, 1390, 366, 878, 1392, 370, 374, 376, 889, 378, 1920, 1408, 1413, 391, 904, 903, 394, 396, 909, 911, 402, 403, 405, 407, 1431, 413, 928, 1447, 424, 423, 428, 429, 433, 945, 1977, 443, 446, 959, 1470, 1477, 970, 972, 460, 974, 463, 466, 468, 473, 2014, 484, 486, 1001, 1515, 492, 1005, 494, 498, 499, 1524, 502, 508, 1533}\n"]}],"source":["print(louvain_partitions[2])\n","\n","filtered_channels = pd.DataFrame(louvain_partitions[2])"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["filtered_channels = filtered_channels[0].sort_values(ascending=True)\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["3         6\n","4         7\n","5         8\n","6         9\n","7        10\n","       ... \n","10     2061\n","25     2086\n","41     2124\n","95     2218\n","117    2255\n","Name: 0, Length: 243, dtype: int64"]},"metadata":{},"output_type":"display_data"}],"source":["display(filtered_channels)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["filtered_channels.to_csv(\"data/louvain_filtered_channels.csv\", sep=';', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["We filter our graph with the channels that interest us."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def filter_function(n):\n","    return n in louvain_partitions[2]\n","\n","sub_G = nx.subgraph_view(G, filter_node=filter_function)"]},{"cell_type":"markdown","metadata":{},"source":["We run the louvain algorithm on our filtered channels and print how many communities we have detected"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["We have detected 7 communities\n"]}],"source":["louvain_communities = nx_comm.louvain_communities(sub_G, resolution=0.9,threshold=1e-7, seed=1)\n","print(\"We have detected {num} communities\".format(num=len(louvain_communities)))"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Community number 0 has 52 members\n","Community number 1 has 43 members\n","Community number 2 has 42 members\n","Community number 3 has 4 members\n","Community number 4 has 31 members\n","Community number 5 has 52 members\n","Community number 6 has 19 members\n"]}],"source":["for idx in range(len(louvain_communities)):\n","    print(\"Community number {i} has {number} members\".format(i=idx,number=len(louvain_communities[idx])))"]},{"cell_type":"markdown","metadata":{},"source":["## Analysis of the communities\n","\n","We print the channels numbers for each community and list for each community the name of the most known or defining channels we have found.\n","Note that the order in which the communities appear is made randomly by the louvain algorithm. Changing the seed changes the order in which the communities appear, but not the result."]},{"cell_type":"markdown","metadata":{},"source":["### Leaning right\n","\n","We have found these channels : Philip de Franco, Drama Alert, IntMensOrg.\n","\n","We found that these channels were not the most \"serious\" news channels even though some of them are really well known. With some research we have found that they where leaning right and that IntMensOrg is proabably mysogynistic channels."]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":["0     519\n","1     903\n","2     265\n","3      11\n","4    1165\n","dtype: int64"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["pd.Series(list(louvain_communities[0])).head()"]},{"cell_type":"markdown","metadata":{},"source":["### Clearly left\n","\n","We have found these channels : CNN, Vox, MSNBC, The Young Turks which we have found to be clearly polarised to the left. These were more \"serious\" news channels"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["0    257\n","1    770\n","2    517\n","3      6\n","4      8\n","dtype: int64"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["pd.Series(list(louvain_communities[1])).head()"]},{"cell_type":"markdown","metadata":{},"source":["### Far Right\n","\n","In this community we have found Inside Edition, Fox News, Daily Wire, Rebel News. We have found the channels to be far right. Note that Rebel News is a canadian channe but is far right. This shows that far right communities cross borders, but also that we might have to clean our dataset a bit more "]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["0     645\n","1    1800\n","2     264\n","3     904\n","4       9\n","dtype: int64"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["pd.Series(list(louvain_communities[2])).head()"]},{"cell_type":"markdown","metadata":{},"source":["### A little bit of unwanted channels\n","\n","In this community we have found : Pat Condell(a sort of conspirationist), Sky News Australia, National Post.These channels are to be removed. Most of them are not really known and are not in the US. They are canadian, australian.."]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["0    1337\n","1    1524\n","2     617\n","3     407\n","dtype: int64"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["pd.Series(list(louvain_communities[3])).head()"]},{"cell_type":"markdown","metadata":{},"source":["### Business oriented ?\n","\n","In this community we have found : Today, China uncensored, Fox Business. Further investigation is needed. Maybe when we use the whole dataset we will find more meaningful conclusions. This community might be business oriented du to its interest in China (China uncensored speaks about what is happening in China and Fox business talks about china)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["0    1920\n","1     195\n","2     199\n","3     203\n","4    2124\n","dtype: int64"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["pd.Series(list(louvain_communities[4])).head()"]},{"cell_type":"markdown","metadata":{},"source":["### Leaning Left\n","\n","In this group we have found : Truly, ABC News, BBC News, True Crime Daily, Business Insider, New York Times. These channels are all really well known. On allsides they are classified as being either left, leaning left and center. More data might classify this better, if not we will have to investigate."]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":["0    2049\n","1     259\n","2    1413\n","3    1146\n","4     391\n","dtype: int64"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["pd.Series(list(louvain_communities[5])).head()"]},{"cell_type":"markdown","metadata":{},"source":["### International Channels\n","\n","In this community we have found : Al Jazeera (Quatari owned news channel), African Diaspora news, France 24 English, Visual Politik EN. All these channels are in english and might talk about what is happening in the US but might be outside of the scope of what we want to see"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":["0    1408\n","1    1027\n","2     200\n","3     651\n","4     396\n","dtype: int64"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["pd.Series(list(louvain_communities[6])).head()"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["df_tosave = nx.to_pandas_edgelist(sub_G,)\n","df_tosave.to_csv('data/louvain_filtered_graph.csv', sep=';', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.6 ('ada')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"35442165eabc390bfb19e1c2edefa78a9622cc35526435ed65b49d02ecd48a79"}}},"nbformat":4,"nbformat_minor":2}
